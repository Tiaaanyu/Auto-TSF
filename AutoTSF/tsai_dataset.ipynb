{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/miniconda3/envs/liangc/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sktime.datasets import load_airline\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing # Holt-Winters\n",
    "from sktime.forecasting.arima import ARIMA\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.forecasting.sarimax import SARIMAX\n",
    "from sktime.forecasting.fbprophet import Prophet\n",
    "from sktime.forecasting.compose import BaggingForecaster\n",
    "from sktime.forecasting.trend import TrendForecaster\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.forecasting.trend import STLForecaster\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.compose import EnsembleForecaster\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "from tqdm import tqdm\n",
    "from tsai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tsai.basics import *\n",
    "dataset_names = [\n",
    "    \"ETTh1\", \"ETTh2\", \"ETTm1\", \"ETTm2\",\n",
    "    'm4_yearly_dataset',\n",
    "    'm4_quarterly_dataset',\n",
    "    'm4_monthly_dataset',\n",
    "    'm4_weekly_dataset',\n",
    "    'm4_daily_dataset',\n",
    "    'm4_hourly_dataset',\n",
    "    \"nn5_weekly_dataset\",\n",
    "    \"nn5_daily_dataset_without_missing_values\",\n",
    "    'electricity_hourly_dataset', \n",
    "    'electricity_weekly_dataset'\n",
    "    'tourism_yearly_dataset',\n",
    "    'tourism_quarterly_dataset',\n",
    "    'tourism_monthly_dataset'\n",
    "    ]\n",
    "def get_dataset(datasetname):\n",
    "    if \"ETT\" in datasetname:\n",
    "        ts = get_long_term_forecasting_data(datasetname)\n",
    "        ts = ts.values[:, 1:].astype(float)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(ts)\n",
    "        ts = scaler.transform(ts)\n",
    "        train_, _, test_, _ = train_test_split(ts, ts, test_size=0.2, shuffle=False)\n",
    "        train_sd = SlidingWindow(100-8, horizon=8)\n",
    "        test_sd = SlidingWindow(100-8, horizon=8, stride=100)\n",
    "        train = train_sd(train_)\n",
    "        test = test_sd(test_)\n",
    "        test_cat = test_\n",
    "        return train, test, 8, test_cat\n",
    "    if any(dname in datasetname for dname in ['m4', 'nn5', 'tourism', 'electricity']):\n",
    "        ts = get_Monash_forecasting_data(datasetname)\n",
    "        time_series_names = ts.series_name.unique()\n",
    "        ts_data = ts.values[:, 2]\n",
    "        scaler = StandardScaler()\n",
    "        print(\"transforming ...\")\n",
    "        scaler.fit(ts_data[None])\n",
    "        ts.values[:, 2] = scaler.transform(ts_data[None])[0]\n",
    "        sample_datasets = [ts[ts['series_name'] == tsn].values[:, 2:] for tsn in time_series_names]\n",
    "        train_, _, test_, _ = train_test_split(sample_datasets, sample_datasets, test_size=0.2, shuffle=False)\n",
    "        test_cat = np.concatenate(test_, axis=0)\n",
    "        min_len = min([len(sd) for sd in sample_datasets] + [100])\n",
    "        fh = min(8, min_len//3)\n",
    "        print(\"sliding ...\")\n",
    "        sd = SlidingWindow(min_len - fh, horizon=fh, stride=min(min_len, len(ts) // 13837))\n",
    "        print(\"stride: \", min(min_len, len(ts) // 13837))\n",
    "        print(\"min_len: \", min_len)\n",
    "        test_sd = SlidingWindow(min_len - fh, horizon=fh)\n",
    "        train = [np.concatenate(item, axis=0).astype(float)\n",
    "             for item in list(zip(*[sd(x) for x in train_]))]\n",
    "        test = [np.concatenate(item, axis=0).astype(float)\n",
    "             for item in zip(*[adapt(test_sd(x[-min_len:])) for x in test_])]\n",
    "        return train, test, fh, test_cat\n",
    "def adapt(p):\n",
    "    if len(p[1].shape) == 2:\n",
    "        return (p[0], p[1][None])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \"ETTh1\", \"ETTh2\", \n",
    "    \"ETTm1\", \"ETTm2\",\n",
    "    'm4_yearly_dataset',\n",
    "    'm4_quarterly_dataset',\n",
    "    'm4_monthly_dataset',\n",
    "    'm4_weekly_dataset',\n",
    "    'm4_daily_dataset',\n",
    "    'm4_hourly_dataset',\n",
    "    \"nn5_weekly_dataset\",\n",
    "    \"nn5_daily_dataset_without_missing_values\",\n",
    "    'electricity_hourly_dataset', \n",
    "    'electricity_weekly_dataset'\n",
    "    'tourism_yearly_dataset',\n",
    "    'tourism_quarterly_dataset',\n",
    "    'tourism_monthly_dataset'\n",
    "    ]\n",
    "def get_dataset(datasetname):\n",
    "    if \"ETT\" in datasetname:\n",
    "        ts = get_long_term_forecasting_data(datasetname)\n",
    "        ts = ts.values[:, 1:].astype(float)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(ts)\n",
    "        ts = scaler.transform(ts)\n",
    "        train_, _, test_, _ = train_test_split(ts, ts, test_size=0.2, shuffle=False)\n",
    "        train_sd = SlidingWindow(100-8, horizon=8)\n",
    "        test_sd = SlidingWindow(100-8, horizon=8, stride=100)\n",
    "        train = train_sd(train_)\n",
    "        test = test_sd(test_)\n",
    "        test_cat = test_\n",
    "        return train, test, 8, test_cat\n",
    "    if any(dname in datasetname for dname in ['m4', 'nn5', 'tourism', 'electricity']):\n",
    "        ts = get_Monash_forecasting_data(datasetname)\n",
    "        time_series_names = ts.series_name.unique()\n",
    "        ts_data = ts.values[:, 2]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(ts_data[None])\n",
    "        ts.values[:, 2] = scaler.transform(ts_data[None])[0]\n",
    "        sample_datasets = [ts[ts['series_name'] == tsn].values[:, 2:] for tsn in time_series_names]\n",
    "        train_, _, test_, _ = train_test_split(sample_datasets, sample_datasets, test_size=0.2, shuffle=False)\n",
    "        test_cat = np.concatenate(test_, axis=0)\n",
    "        min_len = min([len(sd) for sd in sample_datasets])\n",
    "        fh = min(8, min_len//3)\n",
    "        sd = SlidingWindow(min_len - (min_len//3), horizon=fh)\n",
    "        train = [np.concatenate(item, axis=0).astype(float)\n",
    "             for item in list(zip(*[sd(x) for x in train_]))]\n",
    "        test = [np.concatenate(item, axis=0).astype(float)\n",
    "             for item in zip(*[sd(x[-min_len:]) for x in test_])]\n",
    "        return train, test, fh, test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_epoch(batchs):\n",
    "    return 6918000 // batchs + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasters = [\n",
    "    (\"trend\", PolynomialTrendForecaster()),\n",
    "    (\"naive\", NaiveForecaster())\n",
    "]\n",
    "algs = {\n",
    "    \"exp\": ExponentialSmoothing,\n",
    "    \"ari\": ARIMA,\n",
    "    \"sari\": SARIMAX,\n",
    "    \"a-ets\": AutoETS,\n",
    "    \"bag\": BaggingForecaster,\n",
    "    \"tre\": TrendForecaster,\n",
    "    \"poly\": PolynomialTrendForecaster,\n",
    "    \"stl\": STLForecaster,\n",
    "    \"pro\": Prophet,\n",
    "    \"ens\": EnsembleForecaster\n",
    "}\n",
    "algs_names = list(algs.keys())\n",
    "def test_sktime_method(alg, test, fh, alg_args=()):\n",
    "    fh_ = ForecastingHorizon(range(1, fh + 1), is_relative=True)\n",
    "    train_y = test[0]\n",
    "    test_y = test[1]\n",
    "    len_test = len(train_y)\n",
    "    mapes = []\n",
    "    for i in tqdm(range(len_test)):\n",
    "        train_y_ = train_y[i].transpose()\n",
    "        test_y_ = test_y[i].transpose()\n",
    "        alg_ = algs[alg](*alg_args)\n",
    "        alg_.fit(pd.DataFrame(train_y_))\n",
    "        pred_y = alg_.predict(fh_)\n",
    "        mape_ = mean_absolute_percentage_error(pred_y, test_y_)\n",
    "        mapes.append(mape_)\n",
    "    mape = float(np.mean(mapes))\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsai_algs = [\n",
    " 'InceptionTimePlus',\n",
    " 'InceptionTimePlus62x62',\n",
    " 'InceptionTimeXLPlus',\n",
    " 'MultiInceptionTimePlus',\n",
    " 'MiniRocketPlus',\n",
    " 'RNNPlus',\n",
    " 'LSTMPlus',\n",
    " 'GRUPlus',\n",
    " 'TSTPlus',\n",
    " 'MultiTSTPlus',\n",
    " 'XCM',\n",
    " 'XCMPlus',\n",
    " 'mWDN']\n",
    "def adapt(x, name=\"X\"):\n",
    "    print(name + \": \")\n",
    "    print(x.shape)\n",
    "    # print(x)\n",
    "    return x\n",
    "def test_tsai_methods(alg, train, test, epoch=10, lr=1e-3, truc=0):\n",
    "    train_len, test_len = len(train[0]), len(test[0])\n",
    "    splits = [list(range(train_len if not truc else truc)), \n",
    "        list(range(train_len, train_len + (test_len if not truc else truc)))]\n",
    "    # print(train[0].shape, train[1].shape, test[0].shape, test[1].shape)\n",
    "    X, y = np.concatenate([train[0], test[0]], axis=0),\\\n",
    "         np.concatenate([train[1], test[1]], axis=0)\n",
    "    tfms = [None, TSForecasting()]\n",
    "    batch_tfms = TSStandardize()\n",
    "    fcst = TSForecaster(X, y, splits=splits, path='models', tfms=tfms,\n",
    "        batch_tfms=batch_tfms, bs=512, arch=alg, metrics=lambda x,y:sum(\n",
    "        mean_absolute_percentage_error(xi, yi) for xi, yi in zip(x.cpu().permute(0,2,1), y.cpu().permute(0,2,1))\n",
    "    ))\n",
    "    fcst.fit_one_cycle(epoch, lr)\n",
    "    return fcst.final_record[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_ = test_tsai_methods(\"InceptionTimePlus\", train, test, epoch=calc_epoch(train[0].shape[0]), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, fh, _ = get_dataset('ETTh1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6918500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13837*500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1, 8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: m4_quarterly_dataset\n",
      "converting data to dataframe...\n",
      "...done\n",
      "\n",
      "freq                   : quarterly\n",
      "forecast_horizon       : 8\n",
      "contain_missing_values : False\n",
      "contain_equal_length   : False\n",
      "\n",
      "exploding dataframe...\n",
      "...done\n",
      "\n",
      "\n",
      "data.shape: (2406108, 3)\n",
      "transforming ...\n",
      "sliding ...\n",
      "stride:  24\n",
      "min_len:  24\n"
     ]
    }
   ],
   "source": [
    "[\n",
    "    \"ETTh1\", \"ETTh2\", \"ETTm1\", \"ETTm2\",\n",
    "    'm4_yearly_dataset',\n",
    "    'm4_quarterly_dataset',\n",
    "    'm4_monthly_dataset',\n",
    "    'm4_weekly_dataset',\n",
    "    'm4_daily_dataset',\n",
    "    'm4_hourly_dataset',\n",
    "    \"nn5_weekly_dataset\",\n",
    "    \"nn5_daily_dataset_without_missing_values\",\n",
    "    'electricity_hourly_dataset', \n",
    "    'electricity_weekly_dataset'\n",
    "    'tourism_yearly_dataset',\n",
    "    'tourism_quarterly_dataset',\n",
    "    'tourism_monthly_dataset'\n",
    "    ]\n",
    "train, test, fh, train_cat = get_dataset(\"m4_quarterly_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sktime_method(\"exp\", test, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arch_names = []\n",
    "fail_arch = []\n",
    "for arch in tqdm(all_archs_names):\n",
    "    try:\n",
    "        test_tsai_methods(arch, train, test, 1, truc=10)\n",
    "        new_arch_names.append(arch)\n",
    "    except Exception as e:\n",
    "        fail_arch.append(arch)\n",
    "        print(\"fail: \", arch)\n",
    "        continue\n",
    "print(new_arch_names, fail_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.learner import all_archs_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = test_sktime_method(\"a-ets\", test, fh)\n",
    "print(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.basics import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\" \n",
    "# ts = get_forecasting_time_series(\"Sunspots\").values\n",
    "# ts = get_long_term_forecasting_data(\"ETTh1\")\n",
    "# ts = ts[ts.columns[1:]]\n",
    "# ts = get_Monash_forecasting_data(\"tourism_monthly_dataset\")\n",
    "# ts = ts[ts.columns[2:]]\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(ts)\n",
    "# ts = scaler.transform(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.series_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = SlidingWindow(60, horizon=1)(ts)\n",
    "import pandas as pd\n",
    "ts_df = pd.DataFrame(ts)\n",
    "y_train, y_test = temporal_train_test_split(ts_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = ForecastingHorizon(y_test.index, is_relative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = load_airline()\n",
    "# y_train, y_test = temporal_train_test_split(y)\n",
    "fh = ForecastingHorizon(y_test.index, is_relative=False)\n",
    "forecaster = ExponentialSmoothing()\n",
    "# forecaster = ARIMA()\n",
    "# forecaster = SARIMAX()\n",
    "# forecaster = AutoETS()\n",
    "# forecaster = Prophet() # no\n",
    "# forecaster = BaggingForecaster()\n",
    "# forecaster = TrendForecaster()\n",
    "# forecaster = PolynomialTrendForecaster()\n",
    "# forecaster = STLForecaster()\n",
    "\n",
    "forecasters = [\n",
    "    (\"trend\", PolynomialTrendForecaster()),\n",
    "    (\"naive\", NaiveForecaster())\n",
    "]\n",
    "# forecaster = EnsembleForecaster(forecasters=forecasters)\n",
    "print(y_train.shape)\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = load_airline()\n",
    "y_train, y_test = temporal_train_test_split(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = get_forecasting_time_series(\"Sunspots\").values\n",
    "X, y = SlidingWindow(60, horizon=1)(ts)\n",
    "print(X.shape, y.shape)\n",
    "splits = TimeSplitter(0.2)(y)\n",
    "tfms = [None, TSForecasting()]\n",
    "batch_tfms = TSStandardize()\n",
    "fcst = TSForecaster(X, y, splits=splits, path='models', tfms=tfms,\n",
    "    batch_tfms= batch_tfms, bs=512, arch=\"TSTPlus\", metrics=mape, cbs=ShowGraph())\n",
    "fcst.fit_one_cycle(50, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AirQualityUCI handcraft \n",
    "print(long_term_forecasting_list) # ETT\n",
    "print(Monash_forecasting_list) # M4, NN5, torism\n",
    "print(UCR_multivariate_list) # PEMS-SF\n",
    "# Weathre2k\n",
    "# VISUELLE2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset(datasetname):\n",
    "    if \"m4\" in datasetname:\n",
    "        ts = get_long_term_forecasting_data(datasetname)\n",
    "        ts = ts[ts.columns[2:]]\n",
    "    if \"ETT\" in datasetname:\n",
    "        ts = get_long_term_forecasting_data(datasetname)\n",
    "        ts = ts[st.columns[1:]]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
